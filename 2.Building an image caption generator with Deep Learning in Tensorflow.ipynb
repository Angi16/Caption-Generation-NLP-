{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Building an image caption generator with Deep Learning in Tensorflow\n",
    " \n",
    " \n",
    "In this tutorial, we’ll learn how a convolutional neural network (CNN) and Long Short Term Memory (LSTM) can be combined to create an image caption generator and generate captions for your own images.\n",
    "## Overview\n",
    "\n",
    "*    Introduction to Image Captioning Model Architecture\n",
    "*    Captions as a Search Problem\n",
    "*    Creating Captions in Tensorflow\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "*    Basic understanding of Convolutional Neural Networks\n",
    "*    Basic understanding of LSTM\n",
    "*    Basic understanding of Tensorflow\n",
    "\n",
    "# Introduction to image captioning model architecture\n",
    "## Combining a CNN and LSTM\n",
    "\n",
    "In 2014, researchers from Google released a paper, [Show And Tell: A Neural Image Caption Generator]('https://arxiv.org/pdf/1411.4555.pdf'). At the time, this architecture was state-of-the-art on the MSCOCO dataset. It utilized a CNN + LSTM to take an image as input and output a caption.\n",
    "A CNN-LSTM Image Caption Architecture:\n",
    "![](img/1.png)\n",
    "\n",
    "\n",
    "## Using a CNN for image embedding\n",
    "\n",
    "A convolutional neural network can be used to create a dense feature vector. This dense vector, also called an embedding, can be used as feature input into other algorithms or networks.\n",
    "\n",
    "For an image caption model, this embedding becomes a dense representation of the image and will be used as the initial state of the LSTM.\n",
    "Mapping input to embedding:\n",
    "![](img/2.png)\n",
    "\n",
    "## LSTM\n",
    "\n",
    "An LSTM is a recurrent neural network architecture that is commonly used in problems with temporal dependences. It succeeds in being able to capture information about previous states to better inform the current prediction through its memory cell state.\n",
    "\n",
    "An LSTM consists of three main components: a forget gate, input gate, and output gate. Each of these gates is responsible for altering updates to the cell’s memory state.\n",
    "An unrolled LSTM:\n",
    "![](img/3.png)\n",
    "\n",
    "For a deeper understanding of LSTM’s, visit [Chris Olah’s post]('https://colah.github.io/posts/2015-08-Understanding-LSTMs/').\n",
    "## Prediction with image as initial state\n",
    "\n",
    "In a sentence language model, an LSTM is predicting the next word in a sentence. Similarly, in a character language model, an LSTM is trying to predict the next character, given the context of previously seen characters.\n",
    "\n",
    "\n",
    "Sentence and character model predictions:\n",
    "![](img/4.png)\n",
    "\n",
    "In an image caption model, you will create an embedding of the image. This embedding will then be fed as initial state into an LSTM. This becomes the first previous state to the language model, influencing the next predicted words.\n",
    "\n",
    "At each time-step, the LSTM considers the previous cell state and outputs a prediction for the most probable next value in the sequence. This process is repeated until the end token is sampled, signaling the end of the caption.\n",
    "\n",
    "Sampling characters from an LSTM.:\n",
    "![](img/5.png)\n",
    "## Captions as a search problem\n",
    "\n",
    "Generating a caption can be viewed as a graph search problem. Here, the nodes are words. The edges are the probability of moving from one node to another. Finding the optimal path involves maximizing the total probability of a sentence.\n",
    "\n",
    "Sampling and choosing the most probable next value is a greedy approach to generating a caption. It is computationally efficient, but can lead to a sub-optimal result.\n",
    "\n",
    "Given all possible words, it would not be computationally/space efficient to calculate all possible sentences and determine the optimal sentence. This rules out using a search algorithm such as Depth First Search or Breadth First Search to find the optimal path.\n",
    "![](img/6.png)\n",
    "## Beam Search\n",
    "\n",
    "Beam search is a breadth-first search algorithm that explores the most promising nodes. It generates all possible next paths, keeping only the top N best candidates at each iteration.\n",
    "\n",
    "As the number of nodes to expand from is fixed, this algorithm is space-efficient and allows more potential candidates than a best-first search.\n",
    "Beam search for building a sentence:\n",
    "![](img/7.png)\n",
    "## Review\n",
    "\n",
    "Up to this point, you’ve learned about creating a model architecture to generate a sentence, given an image. This is done by utilizing a CNN to create a dense embedding and feeding this as initial state to an LSTM. Additionally, you’ve learned how to generate better sentences with beam search.\n",
    "\n",
    "In the next section, you’ll learn to generate captions from a pre-trained model in Tensorflow.\n",
    "## Creating captions in Tensorflow\n",
    "\n",
    "\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "├── Dockerfile\n",
    "├── bin\n",
    "│ └── download_model.py\n",
    "├── etc\n",
    "│ ├── show-and-tell-2M.zip\n",
    "│ ├── show-and-tell.pb\n",
    "│ └── word_counts.txt\n",
    "├── imgs\n",
    "│ └── trading_floor.jpg\n",
    "├── medium_show_and_tell_caption_generator\n",
    "│ ├── __init__.py\n",
    "│ ├── caption_generator.py\n",
    "│ ├── inference.py\n",
    "│ ├── model.py\n",
    "│ └── vocabulary.py\n",
    "└── requirements.txt\n",
    "\n",
    "## Environment setup\n",
    "\n",
    "Here, you’ll use Docker to install Tensorflow.\n",
    "\n",
    "Docker is a container platform that simplifies deployment. It solves the problem of installing software dependencies onto different server environments.  To install Docker, run:\n",
    "```\n",
    "curl https://get.docker.com | sh\n",
    "```\n",
    "After installing Docker, you’ll create two files. A requirements.txt for the Python dependencies and a Dockerfile to create your Docker environment.\n",
    "```\n",
    "tensorflow==1.6.0\n",
    "requests==2.18.4\n",
    "```\n",
    "\n",
    "\n",
    "**Dockerfile:**\n",
    "```\n",
    "FROM ubuntu:16.04\n",
    "\n",
    "RUN apt-get update -y --fix-missing\n",
    "RUN apt-get install -y \\\n",
    "    build-essential \\\n",
    "    wget \\\n",
    "    python3 \\\n",
    "    python3-dev \\\n",
    "    python3-numpy \\\n",
    "    python3-pip\n",
    "\n",
    "ADD $PWD/requirements.txt /requirements.txt\n",
    "RUN pip3 install -r /requirements.txt\n",
    "\n",
    "CMD [\"/bin/bash\"]\n",
    "```\n",
    "\n",
    "To build this image, run:\n",
    "```\n",
    "$ docker build -t colemurray/medium-show-and-tell-caption-generator -f Dockerfile .\n",
    "\n",
    "# On MBP, ~ 3mins\n",
    "# Image can be pulled from dockerhub below\n",
    "```\n",
    "If you would like to avoid building from source, the image can be pulled from dockerhub using:\n",
    "\n",
    "```docker pull colemurray/medium-show-and-tell-caption-generator # Recommended```\n",
    "\n",
    "Download the model\n",
    "\n",
    "![](img/9.png)\n",
    "Show and Tell Inference Architecture source\n",
    "\n",
    "Below, you’ll download the model graph and pre-trained weights. These weights are from a training session on the [MSCOCO ]('http://cocodataset.org/#home')dataset for 2MM iterations.\n",
    "\n",
    "```Python\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "\n",
    "model_dict = {\n",
    "    'show-and-tell-2M': '15Juh0gaYR0qv8GjRL1EvsigErdQXTmnt'\n",
    "}\n",
    "\n",
    "\n",
    "def download_and_extract_model(model_name, data_dir):\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    file_id = model_dict[model_name]\n",
    "    destination = os.path.join(data_dir, model_name + '.zip')\n",
    "    if not os.path.exists(destination):\n",
    "        print('Downloading model to %s' % destination)\n",
    "        download_file_from_google_drive(file_id, destination)\n",
    "        with zipfile.ZipFile(destination, 'r') as zip_ref:\n",
    "            print('Extracting model to %s' % data_dir)\n",
    "            zip_ref.extractall(data_dir)\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(file_id, destination):\n",
    "    URL = \"https://drive.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params={'id': file_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token:\n",
    "        params = {'id': file_id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    parser = argparse.ArgumentParser(add_help=True)\n",
    "    parser.add_argument('--model-dir', type=str, action='store', dest='model_dir',\n",
    "                        help='Path to model protobuf graph')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "download_and_extract_model('show-and-tell-2M', args.model_dir)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "To download, run:\n",
    "\n",
    "```docker run -e PYTHONPATH=$PYTHONPATH:/opt/app -v $PWD:/opt/app \\\n",
    "-it colemurray/medium-show-and-tell-caption-generator \\\n",
    "python3 /opt/app/bin/download_model.py \\\n",
    "--model-dir /opt/app/etc\n",
    "```\n",
    "Next, create a model class. This class is responsible for loading the graph, creating image embeddings, and running an inference step on the model.\n",
    "\n",
    "\n",
    "```Python\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ShowAndTellModel(object):\n",
    "    def __init__(self, model_path):\n",
    "        self._model_path = model_path\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "        self._load_model(model_path)\n",
    "        self._sess = tf.Session(graph=tf.get_default_graph())\n",
    "\n",
    "    def _load_model(self, frozen_graph_path):\n",
    "        \"\"\"\n",
    "        Loads a frozen graph\n",
    "        :param frozen_graph_path: path to .pb graph\n",
    "        :type frozen_graph_path: str\n",
    "        \"\"\"\n",
    "\n",
    "        model_exp = os.path.expanduser(frozen_graph_path)\n",
    "        if os.path.isfile(model_exp):\n",
    "            self.logger.info('Loading model filename: %s' % model_exp)\n",
    "            with tf.gfile.FastGFile(model_exp, 'rb') as f:\n",
    "                graph_def = tf.GraphDef()\n",
    "                graph_def.ParseFromString(f.read())\n",
    "                tf.import_graph_def(graph_def, name='')\n",
    "        else:\n",
    "            raise RuntimeError(\"Missing model file at path: {}\".format(frozen_graph_path))\n",
    "\n",
    "    def feed_image(self, encoded_image):\n",
    "        initial_state = self._sess.run(fetches=\"lstm/initial_state:0\",\n",
    "                                       feed_dict={\"image_feed:0\": encoded_image})\n",
    "        return initial_state\n",
    "\n",
    "    def inference_step(self, input_feed, state_feed):\n",
    "        softmax_output, state_output = self._sess.run(\n",
    "            fetches=[\"softmax:0\", \"lstm/state:0\"],\n",
    "            feed_dict={\n",
    "                \"input_feed:0\": input_feed,\n",
    "                \"lstm/state_feed:0\": state_feed,\n",
    "            })\n",
    "return softmax_output, state_output, None\n",
    "```\n",
    "Download the vocabulary\n",
    "\n",
    "When training an LSTM, it is standard practice to tokenize the input. For a sentence model, this means mapping each unique word to a unique numeric id. This allows the model to utilize a softmax classifier for prediction.\n",
    "\n",
    "Below, you’ll download the vocabulary used for the pre-trained model and create a class to load it into memory. Here, the line number represents the numeric id of the token.\n",
    "\n",
    "```# File structure\n",
    "# token num_of_occurrances\n",
    "\n",
    "# on 213612\n",
    "# of 202290\n",
    "# the 196219\n",
    "# in 182598\n",
    "\n",
    "curl -o etc/word_counts.txt https://raw.githubusercontent.com/ColeMurray/medium-show-and-tell-caption-generator/master/etc/word_counts.txt\n",
    "```\n",
    "To store this vocabulary in memory, you’ll create a class responsible for mapping words to ids.\n",
    "```Python\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Vocabulary class for mapping words to ids\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_file_path,\n",
    "                 start_token=\"<S>\",\n",
    "                 end_token=\"</S>\",\n",
    "                 unk_token=\"<UNK>\"):\n",
    "        \"\"\"Initializes the vocabulary.\n",
    "    \n",
    "        Args:\n",
    "          vocab_file_path: File containing the vocabulary, where the tokens are the first\n",
    "            whitespace-separated token on each line (other tokens are ignored) and\n",
    "            the token ids are the corresponding line numbers.\n",
    "          start_token: Special token denoting sequence start.\n",
    "          end_token: Special token denoting sequence end.\n",
    "          unk_token: Special token denoting unknown tokens.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if not os.path.exists(vocab_file_path):\n",
    "            self.logger.exception(\"Vocab file %s not found.\", vocab_file_path)\n",
    "            raise RuntimeError\n",
    "        self.logger.info(\"Initializing vocabulary from file: %s\", vocab_file_path)\n",
    "\n",
    "        with open(vocab_file_path, mode=\"r\") as f:\n",
    "            reverse_vocab = list(f.readlines())\n",
    "        reverse_vocab = [line.split()[0] for line in reverse_vocab]\n",
    "        assert start_token in reverse_vocab\n",
    "        assert end_token in reverse_vocab\n",
    "        if unk_token not in reverse_vocab:\n",
    "            reverse_vocab.append(unk_token)\n",
    "        vocab = dict([(x, y) for (y, x) in enumerate(reverse_vocab)])\n",
    "\n",
    "        self.logger.info(\"Created vocabulary with %d words\" % len(vocab))\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = reverse_vocab\n",
    "\n",
    "        self.start_id = vocab[start_token]\n",
    "        self.end_id = vocab[end_token]\n",
    "        self.unk_id = vocab[unk_token]\n",
    "\n",
    "    def token_to_id(self, token_id):\n",
    "        if token_id in self.vocab:\n",
    "            return self.vocab[token_id]\n",
    "        else:\n",
    "            return self.unk_id\n",
    "\n",
    "    def id_to_token(self, token_id):\n",
    "        if token_id >= len(self.reverse_vocab):\n",
    "            return self.reverse_vocab[self.unk_id]\n",
    "        else:\n",
    "return self.reverse_vocab[token_id]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## Creating a caption generator:\n",
    "\n",
    "To generate captions, first you’ll create a caption generator. This caption generator utilizes beam search to improve the quality of sentences generated.\n",
    "\n",
    "At each iteration, the generator passes the previous state of the LSTM (initial state is the image embedding) and previous sequence to generate the next softmax vector.\n",
    "\n",
    "The top N most probable candidates are kept and utilized in the next inference step. This process continues until either the max sentence length is reached or all sentences have generated the end-of-sentence token.\n",
    "\n",
    "```Python\n",
    "\n",
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Class for generating captions from an image-to-text model.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TopN(object):\n",
    "    \"\"\"Maintains the top n elements of an incrementally provided set.\"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self._n = n\n",
    "        self._data = []\n",
    "\n",
    "    def size(self):\n",
    "        assert self._data is not None\n",
    "        return len(self._data)\n",
    "\n",
    "    def push(self, x):\n",
    "        \"\"\"Pushes a new element.\"\"\"\n",
    "        assert self._data is not None\n",
    "        if len(self._data) < self._n:\n",
    "            heapq.heappush(self._data, x)\n",
    "        else:\n",
    "            heapq.heappushpop(self._data, x)\n",
    "\n",
    "    def extract(self, sort=False):\n",
    "        \"\"\"Extracts all elements from the TopN. This is a destructive operation.\n",
    "    \n",
    "        The only method that can be called immediately after extract() is reset().\n",
    "    \n",
    "        Args:\n",
    "          sort: Whether to return the elements in descending sorted order.\n",
    "    \n",
    "        Returns:\n",
    "          A list of data; the top n elements provided to the set.\n",
    "        \"\"\"\n",
    "        assert self._data is not None\n",
    "        data = self._data\n",
    "        self._data = None\n",
    "        if sort:\n",
    "            data.sort(reverse=True)\n",
    "        return data\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Returns the TopN to an empty state.\"\"\"\n",
    "        self._data = []\n",
    "\n",
    "\n",
    "class Caption(object):\n",
    "    \"\"\"Represents a complete or partial caption.\"\"\"\n",
    "\n",
    "    def __init__(self, sentence, state, logprob, score, metadata=None):\n",
    "        \"\"\"Initializes the Caption.\n",
    "        Args:\n",
    "          sentence: List of word ids in the caption.\n",
    "          state: Model state after generating the previous word.\n",
    "          logprob: Log-probability of the caption.\n",
    "          score: Score of the caption.\n",
    "          metadata: Optional metadata associated with the partial sentence. If not\n",
    "            None, a list of strings with the same length as 'sentence'.\n",
    "        \"\"\"\n",
    "        self.sentence = sentence\n",
    "        self.state = state\n",
    "        self.logprob = logprob\n",
    "        self.score = score\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def __cmp__(self, other):\n",
    "        \"\"\"Compares Captions by score.\"\"\"\n",
    "        assert isinstance(other, Caption)\n",
    "        if self.score == other.score:\n",
    "            return 0\n",
    "        elif self.score < other.score:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    # For Python 3 compatibility (__cmp__ is deprecated).\n",
    "    def __lt__(self, other):\n",
    "        assert isinstance(other, Caption)\n",
    "        return self.score < other.score\n",
    "\n",
    "    # Also for Python 3 compatibility.\n",
    "    def __eq__(self, other):\n",
    "        assert isinstance(other, Caption)\n",
    "        return self.score == other.score\n",
    "\n",
    "\n",
    "class CaptionGenerator(object):\n",
    "    \"\"\"Class to generate captions from an image-to-text model.\n",
    "    This code is a modification of https://github.com/tensorflow/models/blob/master/research/im2txt/im2txt/inference_utils/caption_generator.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 vocab,\n",
    "                 beam_size=3,\n",
    "                 max_caption_length=20,\n",
    "                 length_normalization_factor=0.0):\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.model = model\n",
    "\n",
    "        self.beam_size = beam_size\n",
    "        self.max_caption_length = max_caption_length\n",
    "        self.length_normalization_factor = length_normalization_factor\n",
    "\n",
    "    def beam_search(self, encoded_image):\n",
    "        # Feed in the image to get the initial state.\n",
    "        partial_caption_beam = TopN(self.beam_size)\n",
    "        complete_captions = TopN(self.beam_size)\n",
    "        initial_state = self.model.feed_image(encoded_image)\n",
    "\n",
    "        initial_beam = Caption(\n",
    "            sentence=[self.vocab.start_id],\n",
    "            state=initial_state[0],\n",
    "            logprob=0.0,\n",
    "            score=0.0,\n",
    "            metadata=[\"\"])\n",
    "\n",
    "        partial_caption_beam.push(initial_beam)\n",
    "\n",
    "        # Run beam search.\n",
    "        for _ in range(self.max_caption_length - 1):\n",
    "            partial_captions_list = partial_caption_beam.extract()\n",
    "            partial_caption_beam.reset()\n",
    "            input_feed = np.array([c.sentence[-1] for c in partial_captions_list])\n",
    "            state_feed = np.array([c.state for c in partial_captions_list])\n",
    "\n",
    "            softmax, new_states, metadata = self.model.inference_step(input_feed,\n",
    "                                                                      state_feed)\n",
    "\n",
    "            for i, partial_caption in enumerate(partial_captions_list):\n",
    "                word_probabilities = softmax[i]\n",
    "                state = new_states[i]\n",
    "                # For this partial caption, get the beam_size most probable next words.\n",
    "                words_and_probs = list(enumerate(word_probabilities))\n",
    "                words_and_probs.sort(key=lambda x: -x[1])\n",
    "                words_and_probs = words_and_probs[0:self.beam_size]\n",
    "                # Each next word gives a new partial caption.\n",
    "                for w, p in words_and_probs:\n",
    "                    if p < 1e-12:\n",
    "                        continue  # Avoid log(0).\n",
    "                    sentence = partial_caption.sentence + [w]\n",
    "                    logprob = partial_caption.logprob + math.log(p)\n",
    "                    score = logprob\n",
    "                    if metadata:\n",
    "                        metadata_list = partial_caption.metadata + [metadata[i]]\n",
    "                    else:\n",
    "                        metadata_list = None\n",
    "                    if w == self.vocab.end_id:\n",
    "                        if self.length_normalization_factor > 0:\n",
    "                            score /= len(sentence) ** self.length_normalization_factor\n",
    "                        beam = Caption(sentence, state, logprob, score, metadata_list)\n",
    "                        complete_captions.push(beam)\n",
    "                    else:\n",
    "                        beam = Caption(sentence, state, logprob, score, metadata_list)\n",
    "                        partial_caption_beam.push(beam)\n",
    "            if partial_caption_beam.size() == 0:\n",
    "                # We have run out of partial candidates; happens when beam_size = 1.\n",
    "                break\n",
    "\n",
    "        # If we have no complete captions then fall back to the partial captions.\n",
    "        # But never output a mixture of complete and partial captions because a\n",
    "        # partial caption could have a higher score than all the complete captions.\n",
    "        if complete_captions.size() == 0:\n",
    "            complete_captions = partial_caption_beam\n",
    "\n",
    "return complete_captions.extract(sort=True)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Next, you’ll load the show and tell model and use it with the above caption generator to create candidate sentences. These sentences will be printed along with their log probability.\n",
    "```Python\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from medium_show_and_tell_caption_generator.caption_generator import CaptionGenerator\n",
    "from medium_show_and_tell_caption_generator.model import ShowAndTellModel\n",
    "from medium_show_and_tell_caption_generator.vocabulary import Vocabulary\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "tf.flags.DEFINE_string(\"model_path\", \"\", \"Model graph def path\")\n",
    "tf.flags.DEFINE_string(\"vocab_file\", \"\", \"Text file containing the vocabulary.\")\n",
    "tf.flags.DEFINE_string(\"input_files\", \"\",\n",
    "                       \"File pattern or comma-separated list of file patterns \"\n",
    "                       \"of image files.\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    model = ShowAndTellModel(FLAGS.model_path)\n",
    "    vocab = Vocabulary(FLAGS.vocab_file)\n",
    "    filenames = _load_filenames()\n",
    "\n",
    "    generator = CaptionGenerator(model, vocab)\n",
    "\n",
    "    for filename in filenames:\n",
    "        with tf.gfile.GFile(filename, \"rb\") as f:\n",
    "            image = f.read()\n",
    "        captions = generator.beam_search(image)\n",
    "        print(\"Captions for image %s:\" % os.path.basename(filename))\n",
    "        for i, caption in enumerate(captions):\n",
    "            # Ignore begin and end tokens <S> and </S>.\n",
    "            sentence = [vocab.id_to_token(w) for w in caption.sentence[1:-1]]\n",
    "            sentence = \" \".join(sentence)\n",
    "            print(\"  %d) %s (p=%f)\" % (i, sentence, math.exp(caption.logprob)))\n",
    "\n",
    "\n",
    "def _load_filenames():\n",
    "    filenames = []\n",
    "    for file_pattern in FLAGS.input_files.split(\",\"):\n",
    "        filenames.extend(tf.gfile.Glob(file_pattern))\n",
    "    logger.info(\"Running caption generation on %d files matching %s\",\n",
    "                len(filenames), FLAGS.input_files)\n",
    "    return filenames\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "tf.app.run()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "To generate captions, you’ll need to pass in one or more images to the script.\n",
    "\n",
    "```docker run -v $PWD:/opt/app \\\n",
    "-e PYTHONPATH=$PYTHONPATH:/opt/app \\\n",
    "-it colemurray/medium-show-and-tell-caption-generator  \\\n",
    "python3 /opt/app/medium_show_and_tell_caption_generator/inference.py \\\n",
    "--model_path /opt/app/etc/show-and-tell.pb \\\n",
    "--input_files /opt/app/imgs/trading_floor.jpg \\\n",
    "--vocab_file /opt/app/etc/word_counts.txt\n",
    "```\n",
    "You should see output:\n",
    "\n",
    "Captions for image trading_floor.jpg:\n",
    " 0) a group of people sitting at tables in a room . (p=0.000306)\n",
    " 1) a group of people sitting around a table with laptops . (p=0.000140)\n",
    " 2) a group of people sitting at a table with laptops . (p=0.000069)\n",
    "\n",
    "Generated Caption: a group of people sitting around a table with laptops:\n",
    "\n",
    "![](img/10.png)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "    how a convolutional neural network and LSTM can be combined to generate captions to an image\n",
    "    how to utilize the beam search algorithm to consider multiple captions and select the most probable sentence.\n",
    "\n",
    "Complete code here.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "    Try with your own images\n",
    "    Read the Show and Tell paper\n",
    "    Create an API to serve captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
